# -*- coding: utf-8 -*-
"""4A-HIPOLITO-EXER2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iKNEmIeVtNuF8lC1wKjyuAP1nKu9vwQE

# **OpenCV Installation**
"""

# Commented out IPython magic to ensure Python compatibility.
!apt-get update
!apt-get install -y cmake build-essential pkg-config

!git clone https://github.com/opencv/opencv.git
!git clone https://github.com/opencv/opencv_contrib.git

!mkdir -p opencv/build
# %cd opencv/build
!cmake -D CMAKE_BUILD_TYPE=RELEASE \
        -D CMAKE_INSTALL_PREFIX=/usr/local \
        -D OPENCV_ENABLE_NONFREE=ON \
        -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules \
        -D BUILD_EXAMPLES=OFF ..
!make -j8
!make install

"""#**Task 1 SIFT Feature Extraction**"""

!pip install  opencv-python
!pip install  opencv-contrib-python
import cv2
import matplotlib.pyplot as plt

image = cv2.imread("/content/HIPOLITO.png")
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

sift = cv2.SIFT_create()

keypoints, descriptors = sift.detectAndCompute(gray_image, None)

image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)

plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
plt.title("SIFT KEYPOINTS")
plt.show()

"""# **Task 2 Surf Feature Extraction**"""

import cv2
import matplotlib.pyplot as plt

# Load the image
image = cv2.imread('/content/HIPOLITO.png')  # Replace 'your_image.jpg' with the actual path to your image
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Initialize SURF detector (requires OpenCV contrib package with non-free enabled)
surf = cv2.xfeatures2d.SURF_create()

# Detect keypoints and descriptors
keypoints, descriptors = surf.detectAndCompute(gray_image, None)

# Draw keypoints on the image
image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)

# Display the image with keypoints
plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
plt.title('SURF Keypoints')
plt.show()

"""# **Task 3 ORB Feature Extraction**"""

# Read the image
image = cv2.imread("/content/HIPOLITO.png")

# Convert the image to grayscale
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Initialize the ORB detector
orb = cv2.ORB_create()

# Detect keypoints and descriptors
keypoints, descriptors = orb.detectAndCompute(gray_image, None)

# Draw keypoints on the image
image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)

# Display the result
plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
plt.title("ORB KEYPOINTS")
plt.show()

"""# **Task 4 Feature Matching**"""

# Load two images
image1 = cv2.imread('/content/HIPOLITO.png', 0)
image2 = cv2.imread('/content/unnamed.jpg', 0)

# Initialize SIFT detector
sift = cv2.SIFT_create()

# Find keypoints and descriptors with SIFT
keypoints1, descriptors1 = sift.detectAndCompute(image1, None)
keypoints2, descriptors2 = sift.detectAndCompute(image2, None)

# Initialize the matcher
bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)

# Match descriptors
matches = bf.match(descriptors1, descriptors2)

# Sort matches by distance (best matches first)
matches = sorted(matches, key=lambda x: x.distance)

# Draw matches
image_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

# Display the matches
plt.imshow(image_matches)
plt.title('Feature Matching with SIFT')
plt.show()

"""# **Task 5 Applications of Feature Matching**"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load two images in grayscale
image1 = cv2.imread('/content/HIPOLITO.png', cv2.IMREAD_GRAYSCALE)
image2 = cv2.imread('/content/unnamed.jpg', cv2.IMREAD_GRAYSCALE)

# Detect keypoints and descriptors using SIFT
sift = cv2.SIFT_create()
keypoints1, descriptors1 = sift.detectAndCompute(image1, None)
keypoints2, descriptors2 = sift.detectAndCompute(image2, None)

# Initialize the matcher
bf = cv2.BFMatcher()
matches = bf.knnMatch(descriptors1, descriptors2, k=2)

# Apply ratio test to find good matches
good_matches = []
for m, n in matches:
    if m.distance < 0.75 * n.distance:  # Adjust 0.75 as needed
        good_matches.append(m)

# Extract location of good matches
if len(good_matches) > 4:  # Check for sufficient good matches
    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

    # Find homography matrix with adjusted RANSAC threshold
    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 10.0)

    # Warp one image to align with the other
    h, w = image1.shape
    result = cv2.warpPerspective(image1, M, (w, h), flags=cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR)

    # Display the result
    plt.imshow(result, cmap='gray')
    plt.title('Enhanced Image Alignment using Homography')
    plt.axis('off')
    plt.show()
else:
    print(f"Not enough good matches were found: {len(good_matches)}/4")

"""# **Combining Feature Extraction Methods**"""

image1 = cv2.imread('/content/HIPOLITO.png', cv2.IMREAD_GRAYSCALE)
image2 = cv2.imread('/content/unnamed.jpg', cv2.IMREAD_GRAYSCALE)

if image1 is None or image2 is None:
    print("Error: Could not load images. Please check the file paths.")
    exit()


sift = cv2.SIFT_create()
keypoints1_sift, descriptors1_sift = sift.detectAndCompute(image1, None)
keypoints2_sift, descriptors2_sift = sift.detectAndCompute(image2, None)


orb = cv2.ORB_create()
keypoints1_orb, descriptors1_orb = orb.detectAndCompute(image1, None)
keypoints2_orb, descriptors2_orb = orb.detectAndCompute(image2, None)